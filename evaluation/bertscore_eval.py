#!/usr/bin/env python3
import json
import argparse
from pathlib import Path
from tqdm import tqdm
from bert_score import score as bert_score
import pandas as pd


def load_jsonl(path: Path):
    """Load JSONL file and return list of dicts."""
    records = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                try:
                    records.append(json.loads(line))
                except:
                    continue
    return records


def compute_bertscore(records, model_type="microsoft/deberta-large-mnli"):
    """
    Compute BERTScore precision/recall/F1 over all items.
    Returns a list of dicts augmented with score fields.
    """
    candidates = [r["answer"] for r in records]
    references = [r["ground truth"] for r in records]

    # BERTScore returns three tensors
    P, R, F1 = bert_score(
        cands=candidates,
        refs=references,
        lang="en",
        model_type=model_type,
        verbose=True
    )

    # Attach scores to each record
    results = []
    for rec, p, r, f in zip(records, P, R, F1):
        out = rec.copy()
        out["bertscore_precision"] = float(p)
        out["bertscore_recall"] = float(r)
        out["bertscore_f1"] = float(f)
        results.append(out)

    return results


def save_results(results, output_jsonl: Path, summary_csv: Path):
    """Save detailed JSONL and summary CSV."""
    # Save detailed per-item scores
    with output_jsonl.open("w", encoding="utf-8") as f:
        for r in results:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

    # Build summary CSV
    df = pd.DataFrame(results)
    summary = df[["bertscore_precision", "bertscore_recall", "bertscore_f1"]].mean()

    summary.to_csv(summary_csv, header=["mean_score"])

    print("\n=== BERTScore Summary ===")
    print(summary)
    print("=========================\n")


def main():
    parser = argparse.ArgumentParser(description="Compute BERTScore for Gemini outputs.")
    parser.add_argument("--gemini-output", type=Path, required=True,
                        help="Path to outputs.jsonl generated by Gemini pipeline")
    parser.add_argument("--output-jsonl", type=Path, default=Path("bertscore_outputs.jsonl"))
    parser.add_argument("--summary-csv", type=Path, default=Path("bertscore_summary.csv"))
    parser.add_argument("--model-type", type=str,
                        default="microsoft/deberta-large-mnli",
                        help="Encoder model to use for BERTScore.")
    args = parser.parse_args()

    records = load_jsonl(args.gemini_output)

    print(f"Loaded {len(records)} Gemini outputs.")
    results = compute_bertscore(records, model_type=args.model_type)

    save_results(results, args.output_jsonl, args.summary_csv)


if __name__ == "__main__":
    main()
